{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Compute Grad using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Official guide:\n",
    "- https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you what compute grad in Pytorch, firstly you should set `requires_grad=True`. Then use `torch.autograd.grad` to obtain grad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `requires_grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sin(a): tensor([0.8415, 0.9093, 0.1411])\n",
      "sin(b): tensor([-0.7568, -0.9589, -0.2794], grad_fn=<SinBackward0>)\n",
      "c: tensor(-0.7440, grad_fn=<SinBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1., 2., 3.])\n",
    "b = torch.tensor([4., 5., 6.], requires_grad=True)\n",
    "\n",
    "print(\"sin(a):\", torch.sin(a))\n",
    "print(\"sin(b):\",torch.sin(b))\n",
    "\n",
    "c = torch.sin(torch.sin(b).sum()**2)\n",
    "print(\"c:\", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `grad_fn` stored with our tensors allows you to walk the computation all the way back to its inputs with its `next_functions` property. We can see below that drilling down on this property on d shows us the gradient functions for all the prior tensors. Note that `b.grad_fn` is reported as None, indicating that this was an input to the function with no history of its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\n",
      "<SinBackward0 object at 0x7fe798a1b160>\n",
      "((<PowBackward0 object at 0x7fe798a1ad70>, 0),)\n",
      "((<SumBackward0 object at 0x7fe63967d3f0>, 0),)\n",
      "((<SinBackward0 object at 0x7fe63967d960>, 0),)\n",
      "((<AccumulateGrad object at 0x7fe63967d3f0>, 0),)\n",
      "()\n",
      "b.grad_fn: None\n"
     ]
    }
   ],
   "source": [
    "print('c:')\n",
    "print(c.grad_fn)\n",
    "print(c.grad_fn.next_functions)\n",
    "print(c.grad_fn.next_functions[0][0].next_functions)\n",
    "print(c.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(c.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(c.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "\n",
    "print(\"b.grad_fn:\", b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `torch.autograd.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8., 10., 12.]),)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "d = torch.sum(b**2)\n",
    "\n",
    "grad(d, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8., 10., 12.]),)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = b**2\n",
    "\n",
    "grad(e, b, grad_outputs=torch.ones_like(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Second Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x)\n",
    "\n",
    "print(grad_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_xx = torch.autograd.grad(outputs=grad_x, inputs=x)\n",
    "# RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason is that pytorch will release the gradient of the nodes created by the intermediate calculation process. So we need to use `create_graph = True` to store our compute graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(12., grad_fn=<AddBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.).requires_grad_()\n",
    "y = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "z = x * x * y\n",
    "\n",
    "grad_x = torch.autograd.grad(outputs=z, inputs=x, create_graph=True)\n",
    "\n",
    "print(grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(6.),)\n"
     ]
    }
   ],
   "source": [
    "grad_xx = torch.autograd.grad(outputs=grad_x, inputs=x)\n",
    "print(grad_xx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
