{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PINN to solve 1D buegers equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Burgers Equation\n",
    "\n",
    "Dne space dimension case:\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t}+u \\frac{\\partial u}{\\partial x}=\\nu \\frac{\\partial^2 u}{\\partial x^2}\n",
    "$$\n",
    "\n",
    "Domain: \n",
    "- space: [-1,1]\n",
    "- time: [0,1]\n",
    "\n",
    "IC/BC condition:\n",
    "\n",
    "$$\n",
    "u(x,0) = sin(\\pi x)\\\\\n",
    "u(-1,t) = u(1, t)=0\n",
    "$$\n",
    "\n",
    "\n",
    "- non-linear\n",
    "- discontinuous solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let see the exact solution solved by numerical method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "DATA_PATH = \"/workspaces/pytorch-PINN/data/center_sine_burgers.npy\"\n",
    "np_data = np.load(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_burgers(np_data, t_idx, num_x=500):\n",
    "    plt.plot(np_data[t_idx*num_x:(t_idx+1)*num_x, 1], np_data[t_idx*num_x:(t_idx+1)*num_x, 2], label='Numerical')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('u')\n",
    "    plt.title(f'Burgers Equation Solution at t={np_data[t_idx*num_x, 0]:.2f}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_burgers(np_data, t_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PINN\n",
    "\n",
    "Physics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process, and can be described by partial differential equations (PDEs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PINN](./pictures/Xtfc_scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PINN with Pytorch\n",
    "\n",
    "To implement a Physics-Informed Neural Network (PINN) using PyTorch, we can divide the code into several functional sections. This allows for a more organized and structured approach to the implementation process:\n",
    "\n",
    "1. Data section:\n",
    "    - Generate data in constrain. (In most cases: initial part, boundary part, interior part) \n",
    "2. Constrain:\n",
    "    - Compute loss.\n",
    "3. Model:\n",
    "    - Setup neural network to represent pde solver.\n",
    "4. Train: \n",
    "    - training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data section\n",
    "\n",
    "In order to solve the equation, we need to prepare three sets of points, which are sampled from the domain of the equation. This will allow us to accurately represent the solution space and obtain accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating data for a PDE by randomly sampling points within specified ranges for the $x$ and $t$ coordinates. The data dictionary contains three keys: \"interior\", \"boundary\", and \"initial\", each of which corresponds to a different type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy implementation\n",
    "\n",
    "x_range = [-1, 1]\n",
    "t_range = [0, 1]\n",
    "\n",
    "def get_data():\n",
    "    # Define the interior, boundary, and initial data for the PDE\n",
    "    data = {}\n",
    "\n",
    "    # Interior data is randomly generated within the specified x and t ranges\n",
    "    data[\"interior\"] = {\n",
    "        \"x\": np.random.uniform(*x_range, 100),\n",
    "        \"t\": np.random.uniform(*t_range, 100),\n",
    "    }\n",
    "\n",
    "    # Boundary data is randomly generated along the x-axis within the specified x range, and at a random time within the specified t range\n",
    "    data[\"boundary\"] = {\n",
    "        \"x\": np.random.choice(x_range, 10),\n",
    "        \"t\": np.random.uniform(*t_range, 10),\n",
    "    }\n",
    "\n",
    "    # Initial data is randomly generated along the x-axis within the specified x range, and at the initial time (t = t_range[0])\n",
    "    data[\"initial\"] = {\n",
    "        \"x\": np.random.uniform(*x_range, 10),\n",
    "        \"t\": np.ones(10) * t_range[0],\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"interior\" data is generated by randomly sampling 100 points within the specified $x$ and $t$ ranges. The \"boundary\" data is generated by randomly sampling 10 points along the $x$-axis within the specified $x$ range, and at a random time within the specified $t$ range. The \"initial\" data is generated by randomly sampling 10 points along the $x$-axis within the specified $x$ range, and at the initial time $t = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show shape and keys in data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree\n",
    "\n",
    "data = get_data()\n",
    "tree.map_structure(lambda x: x.shape, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constrain(loss) section\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines a set of loss functions that are used to train a machine learning model for solving partial differential equations (PDEs). The loss functions are defined as Python functions that take in the predicted solution `u` and the input data `x` and `t`, which represent the spatial and temporal coordinates of the PDE.\n",
    "\n",
    "The first function, `l2_loss`, computes the mean squared error between the predicted solution and the true solution. This is a common loss function used in regression problems.\n",
    "\n",
    "The next three functions, `bcloss`, `icloss`, and `eqloss`, compute the loss for the boundary conditions, initial conditions, and interior equations, respectively. The `bcloss` function computes the difference between the predicted solution and a constant value of 0 at the boundary points. The `icloss` function computes the difference between the predicted solution and a sine wave at the initial time. The `eqloss` function computes the difference between the predicted solution and the right-hand side of the PDE.\n",
    "\n",
    "The final function, `loss`, combines the individual loss functions into a total loss using a weighted sum. The weights for each type of input are specified in the `loss_weights` dictionary. The `loss` function takes in a dictionary of inputs, which includes the boundary conditions, initial conditions, and interior equations. The `loss` function computes the loss for each type of input and combines them into a total loss using a weighted sum. The total loss is returned along with a dictionary of individual losses for each type of input.\n",
    "\n",
    "Overall, these loss functions are used to train a machine learning model to solve a given PDE. The model is trained to predict the solution to the PDE at a set of time and space coordinates. The loss function is used to optimize the model parameters to minimize the difference between the predicted solution and the true solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def l2_norm(x):\n",
    "    return torch.mean(x ** 2)\n",
    "\n",
    "def bcloss(u, x, t):\n",
    "    return l2_norm(u - 0)\n",
    "\n",
    "def icloss(u, x, t):\n",
    "    return l2_norm(u - torch.sin(torch.pi * x))\n",
    "\n",
    "nu = 0.01 / torch.pi\n",
    "\n",
    "def eqloss(u, x, t):\n",
    "    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    return l2_norm(u_t + u * u_x - nu * u_xx)\n",
    "\n",
    "\n",
    "loss_weights = {\"boundary\": 1.0, \"initial\": 1.0, \"interior\": 1.0}\n",
    "def loss(fn, data):\n",
    "    u = {k: fn(**v) for k, v in data.items()}\n",
    "    loss_dict = {\n",
    "        \"boundary\": bcloss(u[\"boundary\"], **data[\"boundary\"]),\n",
    "        \"initial\": icloss(u[\"initial\"], **data[\"initial\"]),\n",
    "        \"interior\": eqloss(u[\"interior\"], **data[\"interior\"]),\n",
    "    }\n",
    "    total_loss = sum([loss_dict[k] * loss_weights[k] for k in loss_dict.keys()])\n",
    "\n",
    "    return total_loss, loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a neural network model for solving partial differential equations (PDEs) using PyTorch. The model is a multi-layer perceptron (MLP) that takes in two inputs, `t` and `x`, which represent the temporal and spatial coordinates of the PDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 1),\n",
    ")\n",
    "\n",
    "def u_sol(t, x):\n",
    "    u = model(torch.cat([t[:, None], x[:, None]], dim=-1))\n",
    "    return u.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is defined using the `nn.Sequential` class, which allows us to define a sequence of layers that are applied to the input data in order.\n",
    "\n",
    "The MLP consists of three fully connected layers, each followed by a hyperbolic tangent activation function. The first layer takes in two inputs, `t` and `x`, and outputs 32 hidden units. The second layer also has 32 hidden units, and the final layer outputs a single value, which represents the predicted solution to the PDE at the given time and space coordinates.\n",
    "\n",
    "The `u_sol` function takes in two arguments, `t` and `x`, which represent the time and space coordinates of the PDE. The function uses the trained model to predict the solution to the PDE at the given coordinates. The `torch.cat()` function is used to concatenate the time and space coordinates along the last dimension, and the `squeeze()` method is used to remove any extra dimensions from the output.\n",
    "\n",
    "Overall, this code defines a neural network model for solving PDEs and provides a function for using the trained model to predict the solution to a given PDE at a set of time and space coordinates. The MLP architecture can be modified to suit different types of PDEs, and the `u_sol` function can be used to evaluate the performance of the model on a given PDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training\n",
    "\n",
    "First define optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(tensor_dict, requires_grad=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Set the requires_grad attribute of all tensors in a nested dictionary.\n",
    "\n",
    "    Args:\n",
    "        tensor_dict (dict): A nested dictionary containing tensors.\n",
    "        requires_grad (bool): Whether to require gradient computation or not.\n",
    "        device (str): The device to move tensors to.\n",
    "    \"\"\"\n",
    "    _tensor_dict = {}\n",
    "    for k, v in tensor_dict.items():\n",
    "        if isinstance(v, dict):\n",
    "            _tensor_dict[k] = set_requires_grad(v, requires_grad, device=device)\n",
    "        else:\n",
    "            # _tensor_dict[k] = v.clone().detach().requires_grad_(True).to(device)\n",
    "            _tensor_dict[k] = torch.tensor(\n",
    "                v,dtype=torch.float, device=device, requires_grad=requires_grad\n",
    "            )\n",
    "    del tensor_dict\n",
    "    return _tensor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()\n",
    "for i in range(100):\n",
    "    inputs = set_requires_grad(get_data())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    _loss, loss_dict = loss(u_sol, inputs)\n",
    "    _loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    _loss_dict = {k: v.item() for k, v in loss_dict.items()}\n",
    "    print(f\"Step {i}, total loss {_loss.item():.4f}, loss_dict {_loss_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above shows a training loop for a machine learning model that solves partial differential equations (PDEs). The loop iterates over a fixed number of steps (in this case, 100), and updates the model parameters at each step using the Adam optimizer.\n",
    "\n",
    "At each step of the loop, a new set of input data is generated using the `get_data()` function. The `set_requires_grad()` function is then called to set the `t` and `x` inputs to require gradients, which allows the optimizer to update their values during training.\n",
    "\n",
    "The optimizer's `zero_grad()` method is called to reset the gradients of all model parameters to zero. The `loss()` function is then called to compute the loss function for the current set of input data. The loss function returns both the total loss and a dictionary of individual losses for each type of input data (boundary conditions, initial conditions, and interior equations).\n",
    "\n",
    "The `backward()` method is called on the total loss to compute the gradients of all model parameters with respect to the loss. The optimizer's `step()` method is then called to update the model parameters based on the gradients and the specified hyperparameters.\n",
    "\n",
    "Finally, the total loss and individual losses are printed to the console for monitoring the training process. The total loss is printed as a floating-point number, and the individual losses are printed as a dictionary of floating-point numbers.\n",
    "\n",
    "Overall, this code shows a basic training loop for a machine learning model that solves PDEs. The loop can be modified to suit different types of PDEs and input data, and the optimizer's hyperparameters can be adjusted to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 500)\n",
    "t = torch.linspace(0, 1, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell is used to evaluate the trained neural network model on a given set of time and space coordinates. Specifically, the code computes the predicted solution to a PDE at a single time point, `t[t_idx]`, and a set of spatial coordinates, `x`.\n",
    "\n",
    "The `u_sol` function is called with two arguments: a tensor of ones with the same shape as `x` multiplied by `t[t_idx]`, and `x` itself. This creates a tensor of shape `(n_points,)`, where `n_points` is the number of spatial coordinates in `x`. The `u_sol` function uses the trained neural network model to predict the solution to the PDE at each of these spatial coordinates and the given time point.\n",
    "\n",
    "The predicted solution is returned as a tensor of shape `(n_points,)`, which represents the predicted values of the solution at each of the spatial coordinates. This tensor is assigned to the variable `u`.\n",
    "\n",
    "Overall, this code is a simple example of how to use the trained neural network model to predict the solution to a PDE at a given set of time and space coordinates. The `u_sol` function can be called with different time and space coordinates to evaluate the model on different parts of the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_idx = 0\n",
    "\n",
    "u = u_sol(torch.ones_like(x)*t[t_idx], x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to plot output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, u.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
